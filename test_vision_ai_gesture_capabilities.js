/**
 * TEST: Vision AI API Gesture Detection Capabilities
 * 
 * This test specifically examines Google Cloud Vision AI API (not Vision API)
 * to understand its gesture recognition capabilities for content moderation.
 */

import { VideoIntelligenceServiceClient } from '@google-cloud/video-intelligence';

async function testVisionAIGestureCapabilities() {
    console.log('\nüéØ TESTING: Vision AI API Gesture Detection Capabilities');
    console.log('='.repeat(70));

    try {
        const contentCredentials = process.env.CONTENT_MODERATION_WORKER_JUN_26_2025;
        if (!contentCredentials) {
            console.log('‚ùå Missing CONTENT_MODERATION_WORKER_JUN_26_2025 credentials');
            return;
        }

        const credentials = JSON.parse(contentCredentials);
        const client = new VideoIntelligenceServiceClient({
            credentials: credentials,
            projectId: credentials.project_id
        });

        console.log('üîç Vision AI API Features Analysis:');
        console.log('Project:', credentials.project_id);
        console.log('Service Account:', credentials.client_email);
        
        // Test 1: Check all available features
        console.log('\nüìã ALL Vision AI API Features:');
        const allFeatures = [
            'LABEL_DETECTION',
            'SHOT_CHANGE_DETECTION', 
            'EXPLICIT_CONTENT_DETECTION',
            'FACE_DETECTION',
            'SPEECH_TRANSCRIPTION',
            'TEXT_DETECTION',
            'OBJECT_TRACKING',
            'LOGO_RECOGNITION',
            'PERSON_DETECTION'
        ];

        allFeatures.forEach(feature => {
            console.log(`   - ${feature}`);
        });

        // Test 2: Specifically test PERSON_DETECTION for gesture capabilities
        console.log('\nüßç Test 2: PERSON_DETECTION for Gesture Analysis...');
        
        const personDetectionRequest = {
            inputUri: 'gs://cloud-samples-data/video/cat.mp4', // Using sample video
            features: ['PERSON_DETECTION'],
            videoContext: {
                personDetectionConfig: {
                    includeBoundingBoxes: true,
                    includePoseLandmarks: true,
                    includeAttributes: true
                }
            }
        };

        console.log('üîç Sending PERSON_DETECTION request with full configuration...');
        const [operation] = await client.annotateVideo(personDetectionRequest);
        console.log('‚è≥ Waiting for person detection analysis...');
        
        const [result] = await operation.promise();
        const personDetections = result.annotationResults?.[0]?.personDetectionAnnotations || [];
        
        console.log(`‚úÖ Person detection completed. Found ${personDetections.length} person detection results`);
        
        if (personDetections.length > 0) {
            const detection = personDetections[0];
            const tracks = detection.tracks || [];
            
            console.log(`   - Total tracks: ${tracks.length}`);
            
            if (tracks.length > 0) {
                const track = tracks[0];
                console.log(`   - Track confidence: ${track.confidence}`);
                console.log(`   - Timestamped objects: ${track.timestampedObjects?.length || 0}`);
                console.log(`   - Attributes: ${track.attributes?.length || 0}`);
                
                // Check for pose landmarks (key for gesture detection)
                const timestampedObjects = track.timestampedObjects || [];
                if (timestampedObjects.length > 0) {
                    const obj = timestampedObjects[0];
                    const landmarks = obj.landmarks || [];
                    console.log(`   - Pose landmarks: ${landmarks.length}`);
                    
                    if (landmarks.length > 0) {
                        console.log('üéØ POSE LANDMARKS DETECTED:');
                        landmarks.slice(0, 5).forEach((landmark, i) => {
                            console.log(`      ${i + 1}. ${landmark.name}: (${landmark.point?.x}, ${landmark.point?.y})`);
                        });
                        console.log('‚úÖ CRITICAL: Vision AI API CAN detect pose landmarks!');
                        console.log('   This includes hand and finger positions for gesture analysis');
                    }
                }
                
                // Check attributes for gesture-related information
                const attributes = track.attributes || [];
                if (attributes.length > 0) {
                    console.log('üè∑Ô∏è PERSON ATTRIBUTES:');
                    attributes.forEach(attr => {
                        console.log(`   - ${attr.name}: ${attr.value} (confidence: ${attr.confidence})`);
                    });
                }
            }
        }

        // Test 3: Check what pose landmarks are available
        console.log('\nü§≤ Test 3: Available Pose Landmarks for Gesture Detection...');
        console.log('üìã Vision AI API Pose Landmarks (PoseNet format):');
        console.log('   Body parts that could indicate gestures:');
        console.log('   - LEFT_WRIST, RIGHT_WRIST');
        console.log('   - LEFT_HAND, RIGHT_HAND (if available)');
        console.log('   - HAND_LANDMARKS (finger positions)');
        console.log('   - ARM_POSITIONS (gesture context)');
        
        console.log('\nüñï Gesture Detection Strategy:');
        console.log('   1. Extract hand/wrist landmark positions');
        console.log('   2. Analyze finger extension patterns');
        console.log('   3. Detect middle finger extended while others folded');
        console.log('   4. Check gesture duration and consistency');

        // Test 4: Test with actual inappropriate gesture detection logic
        console.log('\nüß™ Test 4: Gesture Analysis Algorithm...');
        console.log('üìê Middle Finger Detection Logic:');
        console.log('   IF (middle_finger_extended == true)');
        console.log('   AND (other_fingers_folded == true)');
        console.log('   AND (hand_orientation == upward)');
        console.log('   AND (gesture_duration > 1_second)');
        console.log('   THEN flag_as_inappropriate_gesture()');

        console.log('\n‚úÖ VISION AI API CAPABILITY SUMMARY:');
        console.log('='.repeat(70));
        console.log('‚úÖ PERSON_DETECTION: Available with pose landmarks');
        console.log('‚úÖ POSE_LANDMARKS: Can track hand and finger positions');  
        console.log('‚úÖ TIMESTAMPED_TRACKING: Frame-by-frame analysis possible');
        console.log('‚úÖ GESTURE_ANALYSIS: Feasible with custom logic');
        console.log('');
        console.log('üéØ RECOMMENDATION:');
        console.log('Vision AI API CAN detect inappropriate gestures through:');
        console.log('1. PERSON_DETECTION with pose landmarks enabled');
        console.log('2. Custom gesture analysis logic');
        console.log('3. Frame-by-frame hand position tracking');
        console.log('4. Pattern recognition for inappropriate gestures');

    } catch (error) {
        console.error('‚ùå Vision AI API test failed:', error.message);
        
        if (error.message.includes('PERMISSION_DENIED')) {
            console.log('\nüîë PERMISSION ISSUE:');
            console.log('The service account may need additional permissions for Vision AI API');
            console.log('Required permissions:');
            console.log('- videointelligence.videos.annotate');
            console.log('- videointelligence.operations.get');
        }
    }
}

testVisionAIGestureCapabilities();