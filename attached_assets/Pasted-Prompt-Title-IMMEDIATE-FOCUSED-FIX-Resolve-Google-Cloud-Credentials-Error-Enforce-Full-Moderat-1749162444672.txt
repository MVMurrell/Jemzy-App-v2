Prompt Title: IMMEDIATE & FOCUSED FIX: Resolve Google Cloud Credentials Error & Enforce Full Moderation Pipeline (NO FALLBACKS)

Current Status:
Excellent news: Your backend logs confirm that the Bunny.net upload is now succeeding with a 200 OK! This means the strategy of sending raw WebM directly to Bunny.net correctly handles the EBML header issue and gets the video ingested for playback.

The NEW and CRITICAL Problem:
The "video processing failed after preprocessing" message is now directly linked to the Google Cloud Storage upload failing due to a JSON parsing error with the service account key. This means the GOOGLE_CLOUD_SERVICE_ACCOUNT_KEY environment variable in your Replit project is not correctly formatted or parsed as valid JSON.

CRITICAL DIRECTIVE: REJECT WORKAROUNDS!
I noticed you mentioned attempting to implement a "fallback that still allows the Bunny.net pipeline to work" if Google Cloud fails. THIS IS ABSOLUTELY UNACCEPTABLE. Content moderation is a non-negotiable core feature of this application. Videos MUST pass through Google Cloud Video AI for content safety before being made publicly available.

Do NOT implement any workarounds that bypass or ignore the Google Cloud Video AI moderation step. The solution must be a complete and integrated pipeline.

Specific Instructions for a Focused Fix:

PRIMARY FIX: Resolve GOOGLE_CLOUD_SERVICE_ACCOUNT_KEY Issue:

The error "steam-hous" at the beginning of the JSON parsing error suggests the environment variable is not holding the entire, properly formatted JSON string.
Verify and Correct the Environment Variable:
Go to your Replit project's Secrets (or Environment Variables) section.
Locate GOOGLE_CLOUD_SERVICE_ACCOUNT_KEY.
Ensure its VALUE is the ENTIRE JSON content copied directly from your downloaded Google Cloud service account .json key file. It must start with { and end with } and contain all the key-value pairs (e.g., "type", "project_id", "private_key_id", "private_key", "client_email", etc.).
Do NOT include any file paths, file names, or partial JSON. It must be the raw, complete JSON string.
Double-check for any extra characters, quotes, or accidental truncations.
CONFIRM FULL PIPELINE EXECUTION (NO SKIPPING):

Once the GOOGLE_CLOUD_SERVICE_ACCOUNT_KEY is correctly set, ensure the process_video_in_background function (your asynchronous worker) enforces the following strict order and conditions:
Receive raw WebM from the frontend.
Attempt Upload raw WebM to GCS.
IF THIS FAILS (due to credential issues or any other reason), THE ENTIRE VIDEO PROCESSING FOR THIS VIDEO MUST BE MARKED AS FAILED IN THE DATABASE, AND THE USER NOTIFIED. No fallback to Bunny.net only.
Initiate Google Cloud Video AI analysis on the GCS WebM.
Upload raw WebM to Bunny.net Stream.
Poll Bunny.net Stream for video status until it is status: 2 (ready). (Continue logging this polling process as requested previously).
Wait for Google Cloud Video AI results.
FINAL DECISION: The video is only marked as 'approved' and 'playable' in the database IF:
GCS upload succeeded.
Video AI analysis completed and found no explicit content.
Bunny.net Stream reports status: 2 (ready).
Otherwise (GCS fails, AI flags, or Bunny.net fails to transcode), the video is marked 'flagged' or 'failed'.
PROVIDE BACKEND LOGS FOR THE ATTEMPTED FIX:

After you implement the fix for the GOOGLE_CLOUD_SERVICE_ACCOUNT_KEY, run another test.
Show me the complete backend logs for that new test video. I need to see the success/failure of:
GCS upload.
Google Cloud Video AI call and its result.
Bunny.net Stream upload and its subsequent status polling (confirming status: 2).
FRONTEND UX CORRECTION (Reiteration):

The frontend MUST NOT attempt local playback or display the video as ready/playable until the backend has explicitly confirmed its 'approved' and 'ready' status (after ALL GCS, AI, and Bunny.net steps are complete). It should display "Processing..." during this time.
Focus on fixing the Google Cloud credentials properly and enforcing the complete, end-to-end, conditional pipeline.