I want to write instructions for replit to build out the features based on this policy. When I click on the content moderation menu item in settings I should be taken to a page that has a few different nav options.

For me since I’m the owner I should see an option that says user management. If I click that I should see a page where I can add and remove emails to be approved to see the content moderation option from the settings page from their profile in the Jemzy app. If I add their email it should email them that they have been approved. If they already have a Jemzy account then the feature will be added to their settings page and if they don’t have an account then when they create one with that email the feature will be available right away.
The second option is Comments & Chat. Comments can happen on videos and there are different chat threads in the app. On this page we should see a split between ai flagged content that the user chose to appeal. And there should be a separate section for content flagged by other users.
The third option is flagged Video comments. Video comments can be posted in the video comments section and videos can be posted in group chats. On this page we should see a split between ai flagged content that the user chose to appeal. And there should be a separate section for content flagged by other users.
The fourth option will be for Jem videos. Posted through the main bottom nav red button option.On this page we should see a split between ai flagged content that the user chose to appeal. And there should be a separate section for content flagged by other users.
For all these options a moderator should be able to see the text or video that has been flagged and read it or watch the video. There should be a link to the area in the app it came from so they can get better context. They should be able to accept text or the video if it was a mistake and the end user should be notified. The moderator should be able to reject the comment or video and they have to give a text description of why it was rejected. If the content is rejected by the moderator then the rules stated previously need to execute. 

We need to store the decisions somewhere so we can always access them in case we have to refer back to them. 

If a user deletes a video or text from the app that is in the moderation process then the video or content should delete permanently and the video should be removed from the moderation process.
